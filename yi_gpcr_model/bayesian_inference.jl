using Turing, Distributions, Catalyst, DifferentialEquations
using Plots
using LinearAlgebra
using DynamicPPL: LogDensityFunction, contextualize
using AdvancedMH
using ForwardDiff, Preferences
using StableRNGs
using Serialization
using DynamicPPL

include("model.jl")
include("target_probability.jl")

"""

affine\\_invariant\\_mcmc\\_firstrun(n\\_ensemble: Int64, n\\_walkers": Int64, n\\_iterations: Int64, target\\_probability:DynamicPPL.Model, test\\_case:String) \n

Generate first samples from target\\_probability. Samples n\\_ensembles of n\\_walkers for n\\_iterations using emcee posterior inference algorithm \n
First samples are generated from prior distribution. \n 

Parameters: \n
n\\_ensemble: Int64, number of independent emcee ensembles\n
n\\_walkers": Int64, number of emcee walkers per ensemble\n
n\\_iterations: Int64, number of iterations per walker \n 
target\\_probability:DynamicPPL.Model, target distribution, here, posterior, for sampling. Generated from Turing\\.@model macro\n
test\\_case:String, unregularized vs regularized test case \n

Should return: \n
Nothing. Automatically serializes MCMChain, naming based on test\\_case. 

"""
function affine_invariant_mcmc_firstrun(n_ensemble, n_walkers, n_iterations, target_probability, test_case)
    # wrap probability model so that it is now a LogDensityFunction 
    # we do this because MCMCTempering is compatible with the LogDensityFunction...
    # https://turinglang.org/MCMCTempering.jl/dev/api/
    # https://github.com/TuringLang/MCMCTempering.jl/blob/ed1ca9886d1c49aece23aacf2bc9fcaf77725fd5/src/model.jl#L1-L7

    # using DynamicPPL: LogDensityFunction, contextualize packages
    target_model = LogDensityFunction(target_probability);

    #define priors
    #note, DynamicPPL has an extract_priors function, but this version is not compatible with the Turing in our environment, so I redefine them
    # association rate units are 1/(sec*nM) and and dissociation rate units are 1/sec
    k_1_prior = Uniform(-20,-16) #k_1 = 3.32e-18
    k1_inv_prior = Uniform(-4,0) #kinv = 1e-2 
    k_2_prior = Uniform(-2,2) #k_2=>1.0 (1E0)
    k_3_prior = Uniform(-7,-3) #k_3=>1.0E-5 
    k_4_prior = Uniform(-2,2) #k_4=>4.0 (4E0)
    k_5_prior = Uniform(-6,-2) #k_5=>4.0E-4
    k_6_prior = Uniform(-5,-1) #k_6=>0.0040 (4E-3)
    k_7_prior = Uniform(-3,1) #k_7=>0.11 (1.1E-1)

    #initial protein concentration units are molecules (here, we only infer one initial condition)
    G_prior = Normal(log(7000), 1) #prior initialized about ground-truth initial condition, 7000 molecules

    prior_vector = [k_1_prior, k1_inv_prior, k_2_prior, k_3_prior, k_4_prior, k_5_prior, k_6_prior, k_7_prior, G_prior]

    # Cache to store the samplers
    samplers = Vector{Any}(undef, n_ensemble)

    # Define 4 Ensemble samplers comprised of n_walkers with Stretch proposal
    a = 2.0 #default 2, can tune per problem
    for i=1:n_ensemble
        samplers[i] = Ensemble(n_walkers, StretchProposal(prior_vector, a));
    end

    # Make sure ForwardDiff is in nansafe_mode
    # using ForwardDiff, Preferences
    set_preferences!(ForwardDiff, "nansafe_mode" => true)

    # set rng for reproducibility/different initializations of the chains
    # using StableRNGs
    rngs = [StableRNG(12), StableRNG(25), StableRNG(75), StableRNG(50)];

    # Generate init values to begin sampling for each chain
    previous_chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        previous_chains[i] = [map(x -> rand(rngs[i], x), prior_vector) for j = 1:n_walkers]
    end

    #run sampler
    chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        chains[i] = @timed sample(rngs[i], target_model, samplers[i], n_iterations; chain_type=MCMCChains.Chains, param_names=[ "k_1", "k_1inv", "k_2", "k_3", "k_4", 
        "k_5", "k_6", "k_7", "G"], progress=false, init_params=previous_chains[i])
    end

    #save MCMChain as well as data generated by @timed macro as NamedTuple
    for i in 1:n_ensemble
        serialize(string("outputs/posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_0.jls"), chains[i])
    end

end

"""

affine\\_invariant\\_mcmc(n\\_ensemble: Int64, n\\_walkers": Int64, n\\_iterations: Int64, target\\_probability:DynamicPPL.Model, sub\\_chain: Int64, test\\_case:String) \n

Generate samples from target\\_probability, starting from previous samples. Samples n\\_ensembles of n\\_walkers for n\\_iterations using emcee posterior inference algorithm \n

Parameters: \n
n\\_ensemble: Int64, number of independent emcee ensembles\n
n\\_walkers": Int64, number of emcee walkers per ensemble\n
n\\_iterations: Int64, number of iterations per walker \n 
target\\_probability:DynamicPPL.Model, target distribution, here, posterior, for sampling. Generated from Turing\\.@model macro\n
sub\\_chain: Int64, indicates how many previous samples we've completed. \n
test\\_case:String, unregularized vs regularized test case \n

Should return: \n
Nothing. Automatically serializes MCMChain, naming based on test\\_case. 

"""
function affine_invariant_mcmc(n_ensemble, n_walkers, n_iterations, target_probability, sub_chain, test_case)
    # wrap probability model so that it is now a LogDensityFunction 
    # we do this because MCMCTempering is compatible with the LogDensityFunction...
    # https://turinglang.org/MCMCTempering.jl/dev/api/
    # https://github.com/TuringLang/MCMCTempering.jl/blob/ed1ca9886d1c49aece23aacf2bc9fcaf77725fd5/src/model.jl#L1-L7

    # using DynamicPPL: LogDensityFunction, contextualize packages
    target_model = LogDensityFunction(target_probability);

    #define priors
    #note, DynamicPPL has an extract_priors function, but this version is not compatible with the Turing in our environment, so I redefine them
    # association rate units are 1/(sec*nM) and and dissociation rate units are 1/sec
    k_1_prior = Uniform(-20,-16) #k_1 = 3.32e-18
    k1_inv_prior = Uniform(-4,0) #kinv = 1e-2 
    k_2_prior = Uniform(-2,2) #k_2=>1.0 (1E0)
    k_3_prior = Uniform(-7,-3) #k_3=>1.0E-5 
    k_4_prior = Uniform(-2,2) #k_4=>4.0 (4E0)
    k_5_prior = Uniform(-6,-2) #k_5=>4.0E-4
    k_6_prior = Uniform(-5,-1) #k_6=>0.0040 (4E-3)
    k_7_prior = Uniform(-3,1) #k_7=>0.11 (1.1E-1)

    #initial protein concentration units are molecules (here, we only infer one initial condition)
    G_prior = Normal(log(7000), 1) #prior initialized about ground-truth initial condition, 7000 molecules

    prior_vector = [k_1_prior, k1_inv_prior, k_2_prior, k_3_prior, k_4_prior, k_5_prior, k_6_prior, k_7_prior, G_prior]

    # Cache to store the samplers
    samplers = Vector{Any}(undef, n_ensemble)

    # Define 4 Ensemble samplers comprised of n_walkers with Stretch proposal
    a = 2.0 #default 2, can tune per problem
    for i=1:n_ensemble
        samplers[i] = Ensemble(n_walkers, StretchProposal(prior_vector, a));
    end

    # Make sure ForwardDiff is in nansafe_mode
    # using ForwardDiff, Preferences
    set_preferences!(ForwardDiff, "nansafe_mode" => true)

    # set rng for reproducibility/different initializations of the chains
    # using StableRNGs
    rngs = [StableRNG(12), StableRNG(25), StableRNG(75), StableRNG(50)];

    # Load init values to resume from for each chain using Serialization
    previous_chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        initval = Vector{Any}(undef, n_walkers)
        previous_chains[i] = deserialize(string("outputs/posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_$(sub_chain-1).jls"))[1]
        for j in 1:n_walkers
            initval[j] = Array(previous_chains[i], append_chains=false)[j][end,:]
        end 
        previous_chains[i] = initval
    end

    #run sampler
    chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        chains[i] = @timed sample(rngs[i], target_model, samplers[i], n_iterations; chain_type=MCMCChains.Chains, param_names=[ "k_1", "k_1inv", "k_2", "k_3", "k_4", 
        "k_5", "k_6", "k_7", "G"], progress=false, init_params=previous_chains[i])
    end

    #save MCMChain as well as data generated by @timed macro as NamedTuple
    for i in 1:n_ensemble
        serialize(string("outputs/posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_$(sub_chain).jls"), chains[i])
    end

end
