using Turing, Distributions, Catalyst, DifferentialEquations
using Plots
using LinearAlgebra
using DynamicPPL: LogDensityFunction, contextualize
using AdvancedMH
using ForwardDiff, Preferences
using StableRNGs
using Serialization
using DynamicPPL

include("model.jl")
include("target_probability.jl")

"""

affine\\_invariant\\_mcmc\\_firstrun(n\\_ensemble: Int64, n\\_walkers": Int64, n\\_iterations: Int64, target\\_probability:DynamicPPL.Model, test\\_case:String) \n

Generate first samples from target\\_probability. Samples n\\_ensembles of n\\_walkers for n\\_iterations using emcee posterior inference algorithm \n
First samples are generated from prior distribution. \n 

Parameters: \n
n\\_ensemble: Int64, number of independent emcee ensembles\n
n\\_walkers": Int64, number of emcee walkers per ensemble\n
n\\_iterations: Int64, number of iterations per walker \n 
target\\_probability:DynamicPPL.Model, target distribution, here, posterior, for sampling. Generated from Turing\\.@model macro\n
test\\_case:String, unregularized vs regularized test case \n

Should return: \n
Nothing. Automatically serializes MCMChain, naming based on test\\_case. 

"""
function affine_invariant_mcmc_firstrun(n_ensemble, n_walkers, n_iterations, target_probability, test_case)
    # wrap probability model so that it is now a LogDensityFunction 
    # we do this because MCMCTempering is compatible with the LogDensityFunction...
    # https://turinglang.org/MCMCTempering.jl/dev/api/
    # https://github.com/TuringLang/MCMCTempering.jl/blob/ed1ca9886d1c49aece23aacf2bc9fcaf77725fd5/src/model.jl#L1-L7

    # using DynamicPPL: LogDensityFunction, contextualize packages
    target_model = LogDensityFunction(target_probability);

    #define priors
    #note, DynamicPPL has an extract_priors function, but this version is not compatible with the Turing in our environment, so I redefine them
    # association rate units are 1/(sec*nM) and and dissociation rate units are 1/sec
    k5b_est = Uniform(-6,0) #0.2 
    k5f_est = Uniform(-4,0) #0.06
    k13b_est = Uniform(-6,0) #0.6
    k13f_est = Uniform(-4,0) #0.09  
    k21f_est = Uniform(-4,0) #0.003 
    k21b_est = Uniform(-6,0) #0.1 
    k1f_est = Uniform(-4,0) #0.003
    k1b_est = Uniform(-6,0) #0.06   
    k12f_est = Uniform(-4,0) #0.0015
    k12b_est = Uniform(-6,0) #0.0001  
    k19f_est = Uniform(-4,0) #0.01
    k19b_est = Uniform(-6,0) #0.0214 
    k17f_est = Uniform(-4,0) #0.003
    k17b_est = Uniform(-6,0) #0.1 
    k9b_est = Uniform(-6,0) #0.05    
    k9f_est = Uniform(-4,0) #0.003 
    k10f_est = Uniform(-4,0) #0.01 
    k10b_est = Uniform(-6,0) #0.06 

    prior_vector = [k5b_est, k5f_est, k13b_est, k13f_est, k21f_est, k21b_est, k1f_est, 
    k1b_est, k12f_est, k12b_est, k19f_est, k19b_est, k17f_est, k17b_est, k9b_est, k9f_est, k10f_est, k10b_est]

    # Cache to store the samplers
    samplers = Vector{Any}(undef, n_ensemble)

    # Define 4 Ensemble samplers comprised of n_walkers with Stretch proposal
    a = 2.0 #default 2, can tune per problem
    for i=1:n_ensemble
        samplers[i] = Ensemble(n_walkers, StretchProposal(prior_vector, a));
    end

    # Make sure ForwardDiff is in nansafe_mode
    # using ForwardDiff, Preferences
    set_preferences!(ForwardDiff, "nansafe_mode" => true)

    # set rng for reproducibility/different initializations of the chains
    # using StableRNGs
    rngs = [StableRNG(12), StableRNG(25), StableRNG(75), StableRNG(50)];

    # Generate init values to begin sampling for each chain
    previous_chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        previous_chains[i] = [map(x -> rand(rngs[i], x), prior_vector) for j = 1:n_walkers]
    end

    #run sampler
    chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        chains[i] = @timed sample(rngs[i], target_model, samplers[i], n_iterations; chain_type=MCMCChains.Chains, param_names=[ "k5b_est", "k5f_est", "k13b_est", "k13f_est", "k21f_est", "k21b_est", "k1f_est", 
        "k1b_est", "k12f_est", "k12b_est", "k19f_est", "k19b_est", "k17f_est", "k17b_est", "k9b_est", "k9f_est", "k10f_est", "k10b_est"], progress=false, initial_params=previous_chains[i])
    end

    #save MCMChain as well as data generated by @timed macro as NamedTuple
    for i in 1:n_ensemble
        serialize(string("outputs/003_posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_0.jls"), chains[i])
    end

end

"""

affine\\_invariant\\_mcmc(n\\_ensemble: Int64, n\\_walkers": Int64, n\\_iterations: Int64, target\\_probability:DynamicPPL.Model, sub\\_chain: Int64, test\\_case:String) \n

Generate samples from target\\_probability, starting from previous samples. Samples n\\_ensembles of n\\_walkers for n\\_iterations using emcee posterior inference algorithm \n

Parameters: \n
n\\_ensemble: Int64, number of independent emcee ensembles\n
n\\_walkers": Int64, number of emcee walkers per ensemble\n
n\\_iterations: Int64, number of iterations per walker \n 
target\\_probability:DynamicPPL.Model, target distribution, here, posterior, for sampling. Generated from Turing\\.@model macro\n
sub\\_chain: Int64, indicates how many previous samples we've completed. \n
test\\_case:String, unregularized vs regularized test case \n

Should return: \n
Nothing. Automatically serializes MCMChain, naming based on test\\_case. 

"""
function affine_invariant_mcmc(n_ensemble, n_walkers, n_iterations, target_probability, sub_chain, test_case)
    # wrap probability model so that it is now a LogDensityFunction 
    # we do this because MCMCTempering is compatible with the LogDensityFunction...
    # https://turinglang.org/MCMCTempering.jl/dev/api/
    # https://github.com/TuringLang/MCMCTempering.jl/blob/ed1ca9886d1c49aece23aacf2bc9fcaf77725fd5/src/model.jl#L1-L7

    # using DynamicPPL: LogDensityFunction, contextualize packages
    target_model = LogDensityFunction(target_probability);

    #define priors
    #note, DynamicPPL has an extract_priors function, but this version is not compatible with the Turing in our environment, so I redefine them
    # association rate units are 1/(sec*nM) and and dissociation rate units are 1/sec
    k5b_est = Uniform(-6,0) #0.2 
    k5f_est = Uniform(-4,0) #0.06
    k13b_est = Uniform(-6,0) #0.6
    k13f_est = Uniform(-4,0) #0.09  
    k21f_est = Uniform(-4,0) #0.003 
    k21b_est = Uniform(-6,0) #0.1 
    k1f_est = Uniform(-4,0) #0.003
    k1b_est = Uniform(-6,0) #0.06   
    k12f_est = Uniform(-4,0) #0.0015
    k12b_est = Uniform(-6,0) #0.0001  
    k19f_est = Uniform(-4,0) #0.01
    k19b_est = Uniform(-6,0) #0.0214 
    k17f_est = Uniform(-4,0) #0.003
    k17b_est = Uniform(-6,0) #0.1 
    k9b_est = Uniform(-6,0) #0.05    
    k9f_est = Uniform(-4,0) #0.003 
    k10f_est = Uniform(-4,0) #0.01 
    k10b_est = Uniform(-6,0) #0.06 

    prior_vector = [k5b_est, k5f_est, k13b_est, k13f_est, k21f_est, k21b_est, k1f_est, 
    k1b_est, k12f_est, k12b_est, k19f_est, k19b_est, k17f_est, k17b_est, k9b_est, k9f_est, k10f_est, k10b_est]

    # Cache to store the samplers
    samplers = Vector{Any}(undef, n_ensemble)

    # Define 4 Ensemble samplers comprised of n_walkers with Stretch proposal
    a = 2.0 #default 2, can tune per problem
    for i=1:n_ensemble
        samplers[i] = Ensemble(n_walkers, StretchProposal(prior_vector, a));
    end

    # Make sure ForwardDiff is in nansafe_mode
    # using ForwardDiff, Preferences
    set_preferences!(ForwardDiff, "nansafe_mode" => true)

    # set rng for reproducibility/different ensembles (up to 4)
    # using StableRNGs
    rngs = [StableRNG(12), StableRNG(25), StableRNG(75), StableRNG(50)];

    # Load init values to resume from for each chain using Serialization
    previous_chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        initval = Vector{Any}(undef, n_walkers)
        previous_chains[i] = deserialize(string("outputs/003_posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_$(sub_chain-1).jls")).value
        for j in 1:n_walkers
            initval[j] = Array(previous_chains[i], append_chains=false)[j][end,:]
        end 
        previous_chains[i] = initval
    end

    #run sampler
    chains = Vector{Any}(undef, n_ensemble)
    for i in 1:n_ensemble
        chains[i] = @timed sample(rngs[i], target_model, samplers[i], n_iterations; chain_type=MCMCChains.Chains, param_names=[ "k5b_est", "k5f_est", "k13b_est", "k13f_est", "k21f_est", "k21b_est", "k1f_est", 
        "k1b_est", "k12f_est", "k12b_est", "k19f_est", "k19b_est", "k17f_est", "k17b_est", "k9b_est", "k9f_est", "k10f_est", "k10b_est"], progress=false, initial_params=previous_chains[i])
    end

    #save MCMChain as well as data generated by @timed macro as NamedTuple
    for i in 1:n_ensemble
        serialize(string("outputs/003_posterior_samples_$(test_case)_ensemble$(i)_walkers$(n_walkers)_iter$(n_iterations)_$(sub_chain).jls"), chains[i])
    end

end
